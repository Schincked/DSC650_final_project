{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project Pipeline\n",
    "\n",
    "### Introduction\n",
    "In the fast-paced world of real estate, the ability to continuously monitor and analyze housing data is crucial for obtaining optimal pricing. In order to achieve accurate insights and competitive advantages, clear, reliable, and consistent data is paramount. This project focuses on the creation of a robust data pipeline that facilitates the seamless collection, processing, and analysis of housing data, making it an ideal example of how data engineering and analytics can optimize decision-making processes.\n",
    "\n",
    "### Dataset\n",
    "The dataset used in this project comes from GeeksforGeeks. While smaller in size than what would typically be encountered in a real-world scenario, this dataset provides a clear view of the data we aim to analyze. \n",
    "\n",
    "\n",
    "| **Feature**                      | **Description**                                                                 |\n",
    "|-----------------------------------|---------------------------------------------------------------------------------|\n",
    "| **parcelid**                      | Unique identifier for each property.                                            |\n",
    "| **airconditioningtypeid**         | Identifier for the type of air conditioning system.                             |\n",
    "| **architecturalstyletypeid**      | Identifier for the architectural style of the property.                         |\n",
    "| **basementsqft**                  | Square footage of the basement.                                                  |\n",
    "| **bathroomcnt**                   | Total number of bathrooms in the property.                                      |\n",
    "| **bedroomcnt**                    | Total number of bedrooms in the property.                                       |\n",
    "| **buildingclasstypeid**           | Identifier for the building type (e.g., wood frame, steel).                     |\n",
    "| **buildingqualitytypeid**         | Identifier for the building's quality.                                          |\n",
    "| **calculatedbathnbr**              | Calculated number of bathrooms based on data.                                   |\n",
    "| **decktypeid**                    | Identifier for the type of deck (e.g., wood, concrete).                         |\n",
    "| **finishedfloor1squarefeet**      | Square footage of the first floor that is finished.                             |\n",
    "| **calculatedfinishedsquarefeet**  | Total finished square footage of the property.                                  |\n",
    "| **fips**                          | Federal Information Processing Standards (FIPS) code for region.                |\n",
    "| **fireplacecnt**                  | Number of fireplaces in the property.                                           |\n",
    "| **fullbathcnt**                   | Number of full bathrooms in the property.                                       |\n",
    "| **garagecarcnt**                  | Number of cars the garage can accommodate.                                      |\n",
    "| **garagetotalsqft**               | Total square footage of the garage.                                             |\n",
    "| **hashottuborspa**                | Binary indicator if the property has a hot tub or spa.                          |\n",
    "| **heatingorsystemtypeid**         | Identifier for the heating or climate control system.                           |\n",
    "| **latitude**                      | Geographic latitude of the property.                                            |\n",
    "| **longitude**                     | Geographic longitude of the property.                                           |\n",
    "| **lotsizesquarefeet**             | Size of the property lot in square feet.                                        |\n",
    "| **poolcnt**                       | Number of pools on the property.                                                |\n",
    "| **poolsizesum**                   | Total size (square footage) of all pools.                                       |\n",
    "| **propertycountylandusecode**     | Code for the type of land use in the county.                                    |\n",
    "| **propertylandusetypeid**         | Identifier for the land use type (e.g., residential, commercial).               |\n",
    "| **propertyzoningdesc**            | Description of the zoning classification (e.g., residential, commercial).        |\n",
    "| **regionidcity**                  | Identifier for the city the property is located in.                             |\n",
    "| **regionidcounty**                | Identifier for the county the property is located in.                           |\n",
    "| **regionidzip**                   | ZIP code identifier for the property.                                           |\n",
    "| **roomcnt**                       | Total number of rooms in the property.                                          |\n",
    "| **storytypeid**                   | Identifier for the number of stories in the property.                           |\n",
    "| **threequarterbathnbr**           | Number of three-quarter bathrooms (toilet, sink, shower).                       |\n",
    "| **unitcnt**                       | Number of units in a multi-unit property.                                        |\n",
    "| **yearbuilt**                     | The year the property was built.                                                |\n",
    "| **structuretaxvaluedollarcnt**    | Tax-assessed value of the property structure.                                   |\n",
    "| **taxvaluedollarcnt**             | Total tax-assessed value of the property (land + structure).                    |\n",
    "| **assessmentyear**                | Year of the tax assessment.                                                     |\n",
    "| **landtaxvaluedollarcnt**         | Tax-assessed value of the property land.                                        |\n",
    "| **taxamount**                     | Total property taxes due.                                                       |\n",
    "| **taxdelinquencyflag**            | Binary flag indicating if there is a tax delinquency.                           |\n",
    "| **taxdelinquencyyear**            | Year in which tax delinquency occurred, if applicable.                          |\n",
    "| **target**                        | Target variable for prediction models (e.g., sale status).                      |\n",
    "\n",
    "\n",
    "\n",
    "### Pipeline Overview\n",
    "\n",
    "The data pipeline is designed to automate the ingestion, processing, and analysis of the housing dataset. Below is a high-level overview of the pipeline architecture:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```mermaid\n",
    "graph LR;\n",
    "    subgraph Ingestion\n",
    "        A[GeeksforGeeks Website] --> B[InvokeHTTP Processor]\n",
    "    end\n",
    "    subgraph Transformation\n",
    "        B --> C[PutHDFS Processor]\n",
    "        C --> D[Hadoop File Store]\n",
    "        D --> E[Hive Table Upload]\n",
    "        E --> F[Pyspark/Polars for Analysis]\n",
    "        F --> G[Machine Learning Models/Analysis Output]\n",
    "    end\n",
    "    subgraph Loading\n",
    "        G --> H[Loading to Final Storage/Reporting]\n",
    "    end\n",
    "\n",
    "    style Ingestion fill:#a2d3f7,stroke:#0061f2,stroke-width:2px\n",
    "    style Transformation fill:#f7d2a2,stroke:#f2a600,stroke-width:2px\n",
    "    style Loading fill:#d3f7a2,stroke:#60f200,stroke-width:2px\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The data is ingested into the pipeline via NiFi. NiFi uses the InvokeHTTP processor to get the data from the GeeksforGeeks website that hosts the data. It then uses the PutHDFS processor to put it into a hadoop file store location in data/zillow. Once there I use the Hive CLI to take the data from the file and upload it to a hive table. Once in the Hive table, I can use Pyspark to do machine learning and analysis on the dataset.\n",
    "\n",
    "### Issues Encountered\n",
    "\n",
    "So I did encounter quite a few issues. I do want to clarify that I did not encounter issues with the core pipeline itself, but I did run into issues with Hive integration. I was able to successfully get the Hive Processors in NiFi, but despite me reconfiguring the Hive-site.xml and attempting to bridge the NiFi connection with Hive, it still proved to be unsuccessful.\n",
    "\n",
    "I have tried several attempts where I configured it to be localhost and port forwarding, but still no luck.\n",
    "\n",
    "### Successful Pipeline\n",
    "\n",
    "#### Nifi Ingestion:\n",
    "\n",
    "![nifi.png](attachment:image.png)\n",
    "\n",
    "#### Moving to HDFS\n",
    "\n",
    "![hdfs.png](attachment:image-2.png)\n",
    "\n",
    "### Code\n",
    "\n",
    "### Improvements\n",
    "Option 1 for batches\n",
    "So for improvements on this project that I will most likely do, I would change quite a bit of it to have much easier integration with all these items.\n",
    "\n",
    "I would change my orchestrator to Prefect (I personally like their UI so much better and it is a lot cleaner to work with). I would have ECS and ECR instances generated so that I can deploy a VM with prefect-deployment.yml files so that I can run the virtual environment (most likely going to be UV since it is written in Rust and so much faster) on that instance. Once it is running, I would then have my pipeline pull data directly from the URL in python using something like HTTPX to get the data. I would save that as a dataframe. Use something like Pandera to ensure the data follows the appropriate Schema. I would then use pyspark or polars (depending on the size of the df) to clean/do analysis and then I would use dbt integration that prefect has to set up a profile to push it to whatever schema I define in the profile (could be test or prod). Then I could call deployments on a schedule that Prefect allows me to do.\n",
    "\n",
    "\n",
    "Option 2:\n",
    "Essentially the same thing except I would set up kafka topic to constantly let me stream data as it gets pulled. Then I would set up DBT to be incremental pushes instead of just batch pushes.\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://mermaid.ink/img/CmdyYXBoIExSOwogICAgc3ViZ3JhcGggSW5nZXN0aW9uCiAgICAgICAgQVtHZWVrc2ZvckdlZWtzIFdlYnNpdGVdIC0tPiBCW0ludm9rZUhUVFAgUHJvY2Vzc29yXQogICAgZW5kCiAgICBzdWJncmFwaCBUcmFuc2Zvcm1hdGlvbgogICAgICAgIEIgLS0-IENbUHV0SERGUyBQcm9jZXNzb3JdCiAgICAgICAgQyAtLT4gRFtIYWRvb3AgRmlsZSBTdG9yZV0KICAgICAgICBEIC0tPiBFW0hpdmUgVGFibGUgVXBsb2FkXQogICAgICAgIEUgLS0-IEZbUHlzcGFyay9Qb2xhcnMgZm9yIEFuYWx5c2lzXQogICAgICAgIEYgLS0-IEdbTWFjaGluZSBMZWFybmluZyBNb2RlbHMvQW5hbHlzaXMgT3V0cHV0XQogICAgZW5kCiAgICBzdWJncmFwaCBMb2FkaW5nCiAgICAgICAgRyAtLT4gSFtMb2FkaW5nIHRvIEZpbmFsIFN0b3JhZ2UvUmVwb3J0aW5nXQogICAgZW5kCgogICAgc3R5bGUgSW5nZXN0aW9uIGZpbGw6I2EyZDNmNyxzdHJva2U6IzAwNjFmMixzdHJva2Utd2lkdGg6MnB4CiAgICBzdHlsZSBUcmFuc2Zvcm1hdGlvbiBmaWxsOiNmN2QyYTIsc3Ryb2tlOiNmMmE2MDAsc3Ryb2tlLXdpZHRoOjJweAogICAgc3R5bGUgTG9hZGluZyBmaWxsOiNkM2Y3YTIsc3Ryb2tlOiM2MGYyMDAsc3Ryb2tlLXdpZHRoOjJweAo=\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<img src=\"https://mermaid.ink/img/CmdyYXBoIExSOwogICAgc3ViZ3JhcGggSW5nZXN0aW9uCiAgICAgICAgQVtHZWVrc2ZvckdlZWtzIFdlYnNpdGVdIC0tPiBCW0ludm9rZUhUVFAgUHJvY2Vzc29yXQogICAgZW5kCiAgICBzdWJncmFwaCBUcmFuc2Zvcm1hdGlvbgogICAgICAgIEIgLS0-IENbUHV0SERGUyBQcm9jZXNzb3JdCiAgICAgICAgQyAtLT4gRFtIYWRvb3AgRmlsZSBTdG9yZV0KICAgICAgICBEIC0tPiBFW0hpdmUgVGFibGUgVXBsb2FkXQogICAgICAgIEUgLS0-IEZbUHlzcGFyay9Qb2xhcnMgZm9yIEFuYWx5c2lzXQogICAgICAgIEYgLS0-IEdbTWFjaGluZSBMZWFybmluZyBNb2RlbHMvQW5hbHlzaXMgT3V0cHV0XQogICAgZW5kCiAgICBzdWJncmFwaCBMb2FkaW5nCiAgICAgICAgRyAtLT4gSFtMb2FkaW5nIHRvIEZpbmFsIFN0b3JhZ2UvUmVwb3J0aW5nXQogICAgZW5kCgogICAgc3R5bGUgSW5nZXN0aW9uIGZpbGw6I2EyZDNmNyxzdHJva2U6IzAwNjFmMixzdHJva2Utd2lkdGg6MnB4CiAgICBzdHlsZSBUcmFuc2Zvcm1hdGlvbiBmaWxsOiNmN2QyYTIsc3Ryb2tlOiNmMmE2MDAsc3Ryb2tlLXdpZHRoOjJweAogICAgc3R5bGUgTG9hZGluZyBmaWxsOiNkM2Y3YTIsc3Ryb2tlOiM2MGYyMDAsc3Ryb2tlLXdpZHRoOjJweAo=\" width=\"100%\" />"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import base64\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def mm(graph):\n",
    "    # Encode the Mermaid graph into a base64 string\n",
    "    graphbytes = graph.encode(\"utf8\")\n",
    "    base64_bytes = base64.urlsafe_b64encode(graphbytes)\n",
    "    base64_string = base64_bytes.decode(\"ascii\")\n",
    "    \n",
    "    # Construct the URL for the mermaid diagram\n",
    "    url = \"https://mermaid.ink/img/\" + base64_string\n",
    "    print(url)\n",
    "    # Embed the image using HTML\n",
    "    display(HTML(f'<img src=\"{url}\" width=\"100%\" />'))\n",
    "\n",
    "# Call the function to render the Mermaid diagram\n",
    "mm(\"\"\"\n",
    "graph LR;\n",
    "    subgraph Ingestion\n",
    "        A[GeeksforGeeks Website] --> B[InvokeHTTP Processor]\n",
    "    end\n",
    "    subgraph Transformation\n",
    "        B --> C[PutHDFS Processor]\n",
    "        C --> D[Hadoop File Store]\n",
    "        D --> E[Hive Table Upload]\n",
    "        E --> F[Pyspark/Polars for Analysis]\n",
    "        F --> G[Machine Learning Models/Analysis Output]\n",
    "    end\n",
    "    subgraph Loading\n",
    "        G --> H[Loading to Final Storage/Reporting]\n",
    "    end\n",
    "\n",
    "    style Ingestion fill:#a2d3f7,stroke:#0061f2,stroke-width:2px\n",
    "    style Transformation fill:#f7d2a2,stroke:#f2a600,stroke-width:2px\n",
    "    style Loading fill:#d3f7a2,stroke:#60f200,stroke-width:2px\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dextermain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
